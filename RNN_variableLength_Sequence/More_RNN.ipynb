{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "More_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzT9CLQw_N_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from prepare_data2 import parse_seq\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMDGsU5wGL6r",
        "colab_type": "code",
        "outputId": "94946d7c-a882-4ccc-8b2e-522fed59610d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 prepare_data2.py shakespeare_input.txt shake \\\\n\\\\n+ -m 500"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-04 09:13:03.323831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31022 sequences...\n",
            "Longest sequence is 3094 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "29429 sequences remaining.\n",
            "Longest remaining sequence has length 499.\n",
            "Removing length-0 sequences...\n",
            "29429 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2pM-7KkGujb",
        "colab_type": "code",
        "outputId": "3577181a-ad9c-4fb3-8c16-455db93ff22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "bs = 128\n",
        "seq_len = 500\n",
        "data = tf.data.TFRecordDataset(\"shake.tfrecords\")\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "data = data.shuffle(46000).padded_batch(128, seq_len, drop_remainder=True).repeat()\n",
        "\n",
        "vocab = pickle.load(open(\"shake_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "ch_to_ind = {ch: ind for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab_size)\n",
        "print(vocab)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n",
            "{'w': 3, 'p': 4, ',': 5, 'S': 6, 'i': 7, 'O': 8, 'k': 9, ';': 10, 'n': 11, 'L': 12, 'C': 13, 'j': 14, 'h': 15, 'r': 16, 'W': 17, 'H': 18, 'c': 19, 't': 20, 'g': 21, 'y': 22, 'T': 23, '$': 24, 'A': 25, 'D': 26, 'v': 27, 'M': 28, 'x': 29, '!': 30, '-': 31, 'z': 32, ' ': 33, 'J': 34, 'E': 35, 'd': 36, 'o': 37, 'U': 38, '.': 39, '[': 40, 'B': 41, 'F': 42, 'K': 43, '\\n': 44, 'l': 45, 'f': 46, 'Y': 47, 's': 48, 'V': 49, '3': 50, 'q': 51, 'Z': 52, 'e': 53, 'N': 54, \"'\": 55, 'Q': 56, 'u': 57, ']': 58, 'P': 59, '?': 60, 'X': 61, 'a': 62, 'G': 63, ':': 64, '&': 65, 'I': 66, 'b': 67, 'R': 68, 'm': 69, '<PAD>': 0, '<S>': 1, '</S>': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0ahhz40pYkh",
        "colab_type": "code",
        "outputId": "820f8d73-40ef-4479-ece2-e57ff999be09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "for seq_batch in data:\n",
        "  for seq in seq_batch:\n",
        "    print(seq)\n",
        "    print(\"-----------------------------------\")\n",
        "    inp = tf.one_hot(seq, vocab_size, axis=-1)\n",
        "    oh_seq = inp\n",
        "    print(inp)\n",
        "    to_chars = \"\".join([ind_to_ch[ind] for ind in seq.numpy()])\n",
        "    print(to_chars)\n",
        "    if input() == 'x':\n",
        "      break\n",
        "  break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 1 25 68 66 35 12 64 44 25 22  5 33 48  7 16 39  2  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(500,), dtype=int32)\n",
            "-----------------------------------\n",
            "tf.Tensor(\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]], shape=(500, 70), dtype=float32)\n",
            "<S>ARIEL:\n",
            "Ay, sir.</S><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxIRXp4sHmvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_h = 512\n",
        "\n",
        "layers = [tf.keras.Input(shape=(seq_len, vocab_size), batch_size=128),\n",
        "          tf.keras.layers.SimpleRNN(n_h, return_sequences=True, stateful=True),\n",
        "          tf.keras.layers.Dense(vocab_size)]\n",
        "\n",
        "model = tf.keras.Sequential(layers)\n",
        "\n",
        "\n",
        "steps = 20*35000 // bs\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.build()\n",
        "\n",
        "all_vars = [vars for vars in model.trainable_variables]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYa_brY3JWZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def run_rnn_on_seq(seq_batch):\n",
        "    model.reset_states()\n",
        "    with tf.GradientTape() as tape:\n",
        "        mask_len = [] \n",
        "        mask_len.append(tf.math.count_nonzero(seq, dtype=tf.float32)-1,)\n",
        "\n",
        "        mask_len = tf.stack(mask_len)        \n",
        "        mask = tf.sequence_mask(mask_len, seq_len, dtype=tf.float32)       \n",
        "        \n",
        "\n",
        "        oh_seq  = tf.one_hot(seq_batch, vocab_size, axis=-1)        \n",
        "        y_actual = tf.roll(seq_batch, -1, 1)\n",
        "        \n",
        "\n",
        "        logits = model(oh_seq)        \n",
        "        losses = loss_fn(y_actual, logits)\n",
        "        \n",
        "        \n",
        "        losses = losses * mask        \n",
        "        losses = tf.reduce_sum(losses, axis=1)    # sum up losses for each seq\n",
        "\n",
        "        # average loss over batches\n",
        "        xcent = tf.reduce_mean(tf.math.divide(losses,mask_len))   # dividing the sum of losses for each seq by the seq len\n",
        "\n",
        "    grads = tape.gradient(xcent, all_vars)\n",
        "    \n",
        "    # this is gradient clipping\n",
        "    glob_norm = tf.linalg.global_norm(grads)\n",
        "    grads = [g/glob_norm for g in grads]\n",
        "    \n",
        "    opt.apply_gradients(zip(grads, all_vars))\n",
        "\n",
        "    return xcent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOjGaollNPvP",
        "colab_type": "code",
        "outputId": "bc2fa17d-e0d7-4234-ebff-dcf7cd0a4fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "for step, seqs in enumerate(data):\n",
        "    xent_avg = run_rnn_on_seq(seqs)\n",
        "\n",
        "    if not step % 100:\n",
        "        print(\"Step: {} Loss: {}\".format(step, xent_avg))\n",
        "\n",
        "    if step > steps:\n",
        "        break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 4.18389892578125\n",
            "Step: 100 Loss: 0.5284740924835205\n",
            "Step: 200 Loss: 0.4834766983985901\n",
            "Step: 300 Loss: 0.48588138818740845\n",
            "Step: 400 Loss: 0.49427294731140137\n",
            "Step: 500 Loss: 0.46960213780403137\n",
            "Step: 600 Loss: 0.4127671718597412\n",
            "Step: 700 Loss: 0.4209989309310913\n",
            "Step: 800 Loss: 0.40784698724746704\n",
            "Step: 900 Loss: 0.41049912571907043\n",
            "Step: 1000 Loss: 0.41542983055114746\n",
            "Step: 1100 Loss: 0.4061402678489685\n",
            "Step: 1200 Loss: 0.39543670415878296\n",
            "Step: 1300 Loss: 0.3840625286102295\n",
            "Step: 1400 Loss: 0.39147135615348816\n",
            "Step: 1500 Loss: 0.3904823362827301\n",
            "Step: 1600 Loss: 0.3794107437133789\n",
            "Step: 1700 Loss: 0.4062483310699463\n",
            "Step: 1800 Loss: 0.33503055572509766\n",
            "Step: 1900 Loss: 0.3897464871406555\n",
            "Step: 2000 Loss: 0.3149295449256897\n",
            "Step: 2100 Loss: 0.43651893734931946\n",
            "Step: 2200 Loss: 0.3580419719219208\n",
            "Step: 2300 Loss: 0.3495141565799713\n",
            "Step: 2400 Loss: 0.3237883746623993\n",
            "Step: 2500 Loss: 0.2928924262523651\n",
            "Step: 2600 Loss: 0.3386484384536743\n",
            "Step: 2700 Loss: 0.3663747310638428\n",
            "Step: 2800 Loss: 0.3283182382583618\n",
            "Step: 2900 Loss: 0.3667343258857727\n",
            "Step: 3000 Loss: 0.31864938139915466\n",
            "Step: 3100 Loss: 0.31084397435188293\n",
            "Step: 3200 Loss: 0.3455887734889984\n",
            "Step: 3300 Loss: 0.3587610721588135\n",
            "Step: 3400 Loss: 0.2585485279560089\n",
            "Step: 3500 Loss: 0.2969052791595459\n",
            "Step: 3600 Loss: 0.2782727777957916\n",
            "Step: 3700 Loss: 0.25958847999572754\n",
            "Step: 3800 Loss: 0.36903905868530273\n",
            "Step: 3900 Loss: 0.3344157636165619\n",
            "Step: 4000 Loss: 0.39312100410461426\n",
            "Step: 4100 Loss: 0.2908339202404022\n",
            "Step: 4200 Loss: 0.3159666657447815\n",
            "Step: 4300 Loss: 0.3186192214488983\n",
            "Step: 4400 Loss: 0.3041795790195465\n",
            "Step: 4500 Loss: 0.32921165227890015\n",
            "Step: 4600 Loss: 0.329749196767807\n",
            "Step: 4700 Loss: 0.3439144492149353\n",
            "Step: 4800 Loss: 0.38130176067352295\n",
            "Step: 4900 Loss: 0.3005737364292145\n",
            "Step: 5000 Loss: 0.3329770267009735\n",
            "Step: 5100 Loss: 0.27081766724586487\n",
            "Step: 5200 Loss: 0.3162267208099365\n",
            "Step: 5300 Loss: 0.31069985032081604\n",
            "Step: 5400 Loss: 0.33245164155960083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCScT4MIIanD",
        "colab_type": "code",
        "outputId": "3e87e5cf-3127-4284-8998-d206198ee2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (128, 500, 512)           298496    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (128, 500, 70)            35910     \n",
            "=================================================================\n",
            "Total params: 334,406\n",
            "Trainable params: 334,406\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqYeIhbdKIVj",
        "colab_type": "code",
        "outputId": "eff8c674-40b2-42cf-c232-6216f79a0387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# create new model for generating text; batch size 1, input 1 char \n",
        "\n",
        "layers_gen = [tf.keras.Input(shape=(1,vocab_size), batch_size=1),\n",
        "          tf.keras.layers.SimpleRNN(n_h, return_sequences=True, stateful=True),\n",
        "          tf.keras.layers.Dense(vocab_size)]\n",
        "\n",
        "model_gen = tf.keras.Sequential(layers_gen)\n",
        "\n",
        "model_gen.build()\n",
        "\n",
        "model_gen.set_weights(model.get_weights())\n",
        "\n",
        "model_gen.summary() # same parameters, just different input/output shapes\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_4 (SimpleRNN)     (1, 1, 512)               298496    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, 1, 70)                35910     \n",
            "=================================================================\n",
            "Total params: 334,406\n",
            "Trainable params: 334,406\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qyfpgHJSkMg",
        "colab_type": "code",
        "outputId": "4c7ff8ea-19a8-40d1-d08a-e6df22c084f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def gen_seq(n_seq):\n",
        "  for _ in range(n_seq):\n",
        "    char = 1      \n",
        "    txt = []\n",
        "    while not char == 2:\n",
        "      gen = tf.Variable([[tf.one_hot(char, vocab_size)]])\n",
        "      out = model_gen(gen)\n",
        "      probs = tf.reshape(tf.nn.softmax(out),[-1]).numpy()\n",
        "      char = np.random.choice(vocab_size, p=probs)      \n",
        "      txt.append(char)\n",
        "      if char == 2:\n",
        "        model_gen.reset_states()\n",
        "        break\n",
        "\n",
        "    print(\"\".join([ind_to_ch[ind] for ind in txt]).replace(\"</S>\", \"\\n\\n\"))\n",
        "\n",
        "        \n",
        "gen_seq(50)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DUKE:\n",
            "How Farest thou seemshar, freely.\n",
            "\n",
            "\n",
            "LUCIUS:\n",
            "Here, let impurne you; for I mean to call;\n",
            "And are the king, I know not where not that,\n",
            "In prest as long have a block the string times.\n",
            "\n",
            "\n",
            "LUCEMTENES:\n",
            "And what thon worse, everent at the earth,\n",
            "With streathes the law, belike into her strompe,\n",
            "As year now of that four way?\n",
            "\n",
            "\n",
            "HASTINGS:\n",
            "I think imncal; and then here, robs eish no malice?\n",
            "\n",
            "\n",
            "HORATIOC:\n",
            "But I were blembers with uped by you,\n",
            "Nay, that he is marrate, o' the comforty, shall say\n",
            "Ofter thy showled princely juigna wish thy\n",
            "lords; I prithee, successian, pureow unclary you,\n",
            "Who lies friends, it were done meet. But I chier,\n",
            "Pray our reports is mecarney drum like asit.\n",
            "\n",
            "\n",
            "THESEUS:\n",
            "Well, sir, we have made mescure much is dead!\n",
            "Thy mastless and any, but too make at trait,\n",
            "A deadly oving is ready\n",
            "To sea, a's an exfects you indeed.\n",
            "\n",
            "\n",
            "NERISSA:\n",
            "He's viseared by toads: O, it ceftle\n",
            "But oftrabelling noted, senve and dead?\n",
            "\n",
            "\n",
            "Second Senator:\n",
            "By foom one worthy onky:\n",
            "In valfines ride her left of our prive and say\n",
            "A shome is commirs, and talk us.\n",
            "\n",
            "\n",
            "TITUS ANDRONICUS:\n",
            "Then, 'omive, so long that she shall down, so was mine.\n",
            "\n",
            "\n",
            "DON ADRIANO DE ARMADO:\n",
            "I saw a forethrus help,\n",
            "Whose very severy of foots, and play!\n",
            "My saster the ear and no close to may.\n",
            "\n",
            "\n",
            "FALSTAFF:\n",
            "The lone, and yet come hour her: staren altings,\n",
            "And food with assirnance set on't; it sach.\n",
            "To this new'sle not, spend out his gentleman!\n",
            "Sone.\n",
            "\n",
            "\n",
            "QUEEN MLETET:\n",
            "And you have none but out a place on men!\n",
            "\n",
            "\n",
            "CALIW:\n",
            "Passibl, help, sir: no many way, then, fors!\n",
            "In handres you, opiniquel that I have said.\n",
            "\n",
            "\n",
            "HASTINGS:\n",
            "Out usnagets to do answer.\n",
            "\n",
            "\n",
            "MENENEUS:\n",
            "Doth the letter to inther as the empers,\n",
            "Some born oo passion.\n",
            "\n",
            "\n",
            "BRUTUS:\n",
            "That's bire to swood The stage.\n",
            "\n",
            "\n",
            "PinBit:\n",
            "Some night, I never becade him alroa make deviched.\n",
            "\n",
            "\n",
            "DORDES:\n",
            "This is this streatine I took hath her with speakn.\n",
            "\n",
            "\n",
            "EXETER:\n",
            "Good mother not?\n",
            "\n",
            "\n",
            "IOGBERO:\n",
            "Dive, dognibinus: if you lent your sween to see\n",
            "This I do seamphts drink with what is elveriar\n",
            "Mincy's sorry, be yonces a penuter me,\n",
            "And yet they cannot provised it, I will easion,\n",
            "And, i' the gods ib stald a perform'd honour!\n",
            "\n",
            "\n",
            "CHARMIAN:\n",
            "Let all the court, say! put art their heavy,\n",
            "Suff'd his chread, and this that I'll give you\n",
            "Our neights,\n",
            "I presome to the whepenced but jest,\n",
            "That naw ind all this caulate.\n",
            "\n",
            "\n",
            "MACBETH:\n",
            "He hath been against the call. what arus the mes,\n",
            "Andeed of solair powndneds, for the reason\n",
            "They wart staster, and excell'd her, thou contaiced,\n",
            "Of hands, so line, you shalt hear me a strong you.\n",
            "\n",
            "\n",
            "PANCHINO:\n",
            "I'll do the day yourself a green:\n",
            "Then, doby in heaven come dread to scant fors?\n",
            "\n",
            "\n",
            "MOTH:\n",
            "Wor I did a kinan, by my life and much disposert\n",
            "For sucks with sisters.\n",
            "\n",
            "\n",
            "THERSITES:\n",
            "Ay, Patuen! How!\n",
            "\n",
            "\n",
            "SUFFOLK: Fait--\n",
            "\n",
            "\n",
            "AUTOLYCUS:\n",
            "Now, I must before your horse, I mestend her beard,\n",
            "But with news life belike.\n",
            "\n",
            "\n",
            "EERGOR:\n",
            "As you are younif?\n",
            "\n",
            "\n",
            "OLIVER:\n",
            "How now, youth, as 'ciltie and the garbed majestoust,\n",
            "Who, let oven how Iforace I had a rign,\n",
            "And give him and tloo' your pardon.\n",
            "\n",
            "\n",
            "COUNTESS:\n",
            "I do not scann'd From I'll prove my heart intinius.\n",
            "Hadling speak be not.\n",
            "O, I cause to be made a Ran a rudies are fargetce\n",
            "That I will say my swise--instruring come.\n",
            "To bod I goos.\n",
            "\n",
            "\n",
            "FALSTAFF:\n",
            "I vineny nature, saffty the comformact,\n",
            "Till he you not to give and charge.\n",
            "\n",
            "\n",
            "Second\n",
            "Strong-oney have not so.\n",
            "\n",
            "\n",
            "BARDOLPH:\n",
            "Comes there not, and -enstond? Whew not help:\n",
            "Nose of Gothouth, maky's heaving.\n",
            "\n",
            "\n",
            "ARVIRUGUS:\n",
            "God's sick!\n",
            "\n",
            "\n",
            "BRUTUS:\n",
            "Vels, content I love them sacation,\n",
            "Having me gave a fellow.\n",
            "\n",
            "\n",
            "SIR TABHOTS:\n",
            "I pardon prosperity time herply acloted.\n",
            "\n",
            "\n",
            "CADE:\n",
            "It standly: thy arms thrive notounds in labour.\n",
            "\n",
            "\n",
            "POLPXANE:\n",
            "Fril to thee.\n",
            "\n",
            "\n",
            "MACDUFF:\n",
            "Nay, good night with ashered to us and importion\n",
            "Curlion, the time of loved thou are not ap,\n",
            "Than thy name, by Myself, ad, men the souts, some\n",
            "sholiching as I nouct himself.\n",
            "\n",
            "\n",
            "Boy:\n",
            "If that I trantate as sepend un onous' quoth\n",
            "yee, unpusither cless'd the might between these\n",
            "And the piepatch's this voice of this Haw.\n",
            "\n",
            "\n",
            "MERCULIO:\n",
            "So do you must intene? two after a great menly\n",
            "As so, my horse: doss!\n",
            "\n",
            "\n",
            "KENT:\n",
            "Good her, my friend; knave, I om move england;\n",
            "Ruch remember, let themselves before him.\n",
            "\n",
            "\n",
            "Third Lord:\n",
            "Who know it, less, yet it be myself: trucks,\n",
            "And prince, give you the devil, bring you tull\n",
            "I'll prachis down; but Norforme, and tetry be the intones,\n",
            "Which say you art go sates: one o' dispatch is briefe!\n",
            "\n",
            "\n",
            "Getter:\n",
            "This is, poor with arm drink follia upon my prait\n",
            "soiling brother, unliar that I. Your sons,\n",
            "Till I against the villain age younger, soundly,\n",
            "if you matter not by conscience, butty!\n",
            "You know them; a' winnow him.\n",
            "\n",
            "\n",
            "ODRVINA:\n",
            "Blystes, a mother.\n",
            "\n",
            "\n",
            "First\n",
            "Plone: I, for your owliment thought,\n",
            "And kinds a living me swill; ones you long,\n",
            "And learnee and many goodly rest, silvent!\n",
            "\n",
            "\n",
            "YORK:\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "False forth against much your dost is both;\n",
            "For shall that this very name.\n",
            "\n",
            "\n",
            "MACBETH:\n",
            "He song make keept, or humply the soft.\n",
            "\n",
            "\n",
            "SHALLOW:\n",
            "It shall good saised; founderance,\n",
            "Oldea your'e: a postosi'e: come, indeed, I thank the\n",
            "mouth so strong pleasure us dear millus:\n",
            "A greet the speet that give it in his brother\n",
            "To send them, hell's indeenour, brother, foelly\n",
            "their daybarl! O, i' mother.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww-5Laaiz8BQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_seq_prob(seq):\n",
        "  txt = tf.concat([[1], [ch_to_ind[ch] for ch in seq], [2]],axis=0)\n",
        "  prob_lst = []\n",
        "\n",
        "  char = 1\n",
        "  for ch in txt.numpy():\n",
        "    gen = tf.Variable([[tf.one_hot(ch, vocab_size)]])\n",
        "    out = model_gen(gen)\n",
        "    probs = tf.reshape(-tf.nn.log_softmax(out),[-1]).numpy()\n",
        "    prob_lst.append(probs[ch])\n",
        "  return tf.reduce_sum(prob_lst).numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7AKZ3L-4Ep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5cc2fb0-e118-44df-b170-b882e936d526"
      },
      "source": [
        "seq_1 = \"CAESAR:\\n\\\n",
        "Cowards die many times before their deaths;\\n\\\n",
        "The valiant never taste of death but once.\\n\\\n",
        "Of all the wonders that I yet have heard.\\n\\\n",
        "It seems to me most strange that men should fear;\\n\\\n",
        "Seeing that death, a necessary end,\\n\\\n",
        "Will come when it will come.\\n\\\n",
        "What say the augurers?\"\n",
        "\n",
        "get_seq_prob(seq_1)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2639.7817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGhjHetOTcxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2092c2cd-abee-4ff7-ec41-822514c26b12"
      },
      "source": [
        "# replacing vowels by x\n",
        "\n",
        "seq_2 = \"CXXSXR:\\n\\\n",
        "Cxwxrds dxx mxny txmxs bxfxrx thxxr dxxths;\\n\\\n",
        "Thx vxlxxnt nxvxr txstx xf dxxth bxt xncx.\\n\\\n",
        "xf xll thx wxndxrs thxt x yxt hxvx hxxrd.\\n\\\n",
        "xt sxxms tx mx mxst strxngx thxt mxn shxxld fxxr;\\n\\\n",
        "Sxxxng thxt dxxth, x nxcxssxry xnd,\\n\\\n",
        "Wxll cxmx whxn xt wxll cxmx.\\n\\\n",
        "Whxt sxy thx xxgxrxrs?\"\n",
        "\n",
        "get_seq_prob(seq_2)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3186.086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    }
  ]
}